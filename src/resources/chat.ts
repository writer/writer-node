// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.

import { APIResource } from '../resource';
import { APIPromise } from '../core';
import * as Core from '../core';
import * as ChatAPI from './chat';
import { Stream } from '../streaming';

export class ChatResource extends APIResource {
  /**
   * Chat completion
   */
  chat(body: ChatChatParamsNonStreaming, options?: Core.RequestOptions): APIPromise<Chat>;
  chat(body: ChatChatParamsStreaming, options?: Core.RequestOptions): APIPromise<Stream<ChatStreamingData>>;
  chat(body: ChatChatParamsBase, options?: Core.RequestOptions): APIPromise<Stream<ChatStreamingData> | Chat>;
  chat(
    body: ChatChatParams,
    options?: Core.RequestOptions,
  ): APIPromise<Chat> | APIPromise<Stream<ChatStreamingData>> {
    return this._client.post('/v1/chat', { body, ...options, stream: body.stream ?? false }) as
      | APIPromise<Chat>
      | APIPromise<Stream<ChatStreamingData>>;
  }
}

export interface Chat {
  /**
   * A globally unique identifier (UUID) for the response generated by the API. This
   * ID can be used to reference the specific operation or transaction within the
   * system for tracking or debugging purposes.
   */
  id: string;

  /**
   * An array of objects representing the different outcomes or results produced by
   * the model based on the input provided.
   */
  choices: Array<Chat.Choice>;

  /**
   * The Unix timestamp (in seconds) when the response was created. This timestamp
   * can be used to verify the timing of the response relative to other events or
   * operations.
   */
  created: number;

  /**
   * Identifies the specific model used to generate the response.
   */
  model: string;
}

export namespace Chat {
  export interface Choice {
    /**
     * Describes the condition under which the model ceased generating content. Common
     * reasons include 'length' (reached the maximum output size), 'stop' (encountered
     * a stop sequence), or 'content_filter' (harmful content filtered out).
     */
    finish_reason: 'stop' | 'length' | 'content_filter';

    message: Choice.Message;
  }

  export namespace Choice {
    export interface Message {
      /**
       * The text content produced by the model. This field contains the actual output
       * generated, reflecting the model's response to the input query or command.
       */
      content: string;

      /**
       * Specifies the role associated with the content, indicating whether the message
       * is from the 'assistant' or another defined role, helping to contextualize the
       * output within the interaction flow.
       */
      role: 'user' | 'assistant' | 'system';
    }
  }
}

export interface ChatStreamingData {
  data: Chat;
}

export type ChatChatParams = ChatChatParamsNonStreaming | ChatChatParamsStreaming;

export interface ChatChatParamsBase {
  /**
   * An array of message objects that form the conversation history or context for
   * the model to respond to. The array must contain at least one message.
   */
  messages: Array<ChatChatParams.Message>;

  /**
   * Specifies the model to be used for generating responses. The chat model is
   * always `palmyra-x-002-32k` for conversational use.
   */
  model: string;

  /**
   * Defines the maximum number of tokens (words and characters) that the model can
   * generate in the response. The default value is set to 16, but it can be adjusted
   * to allow for longer or shorter responses as needed.
   */
  max_tokens?: number;

  /**
   * Specifies the number of completions (responses) to generate from the model in a
   * single request. This parameter allows multiple responses to be generated,
   * offering a variety of potential replies from which to choose.
   */
  n?: number;

  /**
   * A token or sequence of tokens that, when generated, will cause the model to stop
   * producing further content. This can be a single token or an array of tokens,
   * acting as a signal to end the output.
   */
  stop?: Array<string> | string;

  /**
   * Indicates whether the response should be streamed incrementally as it is
   * generated or only returned once fully complete. Streaming can be useful for
   * providing real-time feedback in interactive applications.
   */
  stream?: boolean;

  /**
   * Controls the randomness or creativity of the model's responses. A higher
   * temperature results in more varied and less predictable text, while a lower
   * temperature produces more deterministic and conservative outputs.
   */
  temperature?: number;

  /**
   * Sets the threshold for "nucleus sampling," a technique to focus the model's
   * token generation on the most likely subset of tokens. Only tokens with
   * cumulative probability above this threshold are considered, controlling the
   * trade-off between creativity and coherence.
   */
  top_p?: number;
}

export namespace ChatChatParams {
  export interface Message {
    content: string;

    role: 'user' | 'assistant' | 'system';

    name?: string;
  }

  export type ChatChatParamsNonStreaming = ChatAPI.ChatChatParamsNonStreaming;
  export type ChatChatParamsStreaming = ChatAPI.ChatChatParamsStreaming;
}

export interface ChatChatParamsNonStreaming extends ChatChatParamsBase {
  /**
   * Indicates whether the response should be streamed incrementally as it is
   * generated or only returned once fully complete. Streaming can be useful for
   * providing real-time feedback in interactive applications.
   */
  stream?: false;
}

export interface ChatChatParamsStreaming extends ChatChatParamsBase {
  /**
   * Indicates whether the response should be streamed incrementally as it is
   * generated or only returned once fully complete. Streaming can be useful for
   * providing real-time feedback in interactive applications.
   */
  stream: true;
}

export namespace ChatResource {
  export import Chat = ChatAPI.Chat;
  export import ChatStreamingData = ChatAPI.ChatStreamingData;
  export import ChatChatParams = ChatAPI.ChatChatParams;
  export import ChatChatParamsNonStreaming = ChatAPI.ChatChatParamsNonStreaming;
  export import ChatChatParamsStreaming = ChatAPI.ChatChatParamsStreaming;
}

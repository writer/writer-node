// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.

import { maybeFilter } from 'writer-sdk-mcp/filtering';
import { Metadata, asTextContentResult } from 'writer-sdk-mcp/tools/types';

import { Tool } from '@modelcontextprotocol/sdk/types.js';
import Writer from 'writer-sdk';

export const metadata: Metadata = {
  resource: 'completions',
  operation: 'write',
  tags: [],
  httpMethod: 'post',
  httpPath: '/v1/completions',
  operationId: 'completions',
};

export const tool: Tool = {
  name: 'create_completions',
  description:
    "When using this tool, always use the `jq_filter` parameter to reduce the response size and improve performance.\n\nOnly omit if you're sure you don't need the data.\n\nGenerate text completions from a single prompt without conversational context. Best for straightforward text generation tasks like article writing, summaries, or creative content. For interactive conversations or multi-turn dialogues, use generate-chat-completion instead.\n\n# Response Schema\n```json\n{\n  $ref: '#/$defs/completion',\n  $defs: {\n    completion: {\n      type: 'object',\n      properties: {\n        choices: {\n          type: 'array',\n          description: 'A list of choices generated by the model, each containing the text of the completion and associated metadata such as log probabilities.',\n          items: {\n            type: 'object',\n            properties: {\n              text: {\n                type: 'string',\n                description: 'The generated text output from the model, which forms the main content of the response.'\n              },\n              log_probs: {\n                $ref: '#/$defs/logprobs'\n              }\n            },\n            required: [              'text'\n            ]\n          }\n        },\n        model: {\n          type: 'string',\n          description: 'The identifier of the model that was used to generate the responses in the \\'choices\\' array.'\n        }\n      },\n      required: [        'choices'\n      ]\n    },\n    logprobs: {\n      type: 'object',\n      properties: {\n        content: {\n          type: 'array',\n          items: {\n            $ref: '#/$defs/logprobs_token'\n          }\n        },\n        refusal: {\n          type: 'array',\n          items: {\n            $ref: '#/$defs/logprobs_token'\n          }\n        }\n      },\n      required: [        'content',\n        'refusal'\n      ]\n    },\n    logprobs_token: {\n      type: 'object',\n      title: 'logprobs_token',\n      properties: {\n        token: {\n          type: 'string'\n        },\n        logprob: {\n          type: 'number'\n        },\n        top_logprobs: {\n          type: 'array',\n          items: {\n            type: 'object',\n            title: 'top_log_prob',\n            description: 'An array of mappings for each token to its top log probabilities, showing detailed prediction probabilities.',\n            properties: {\n              token: {\n                type: 'string'\n              },\n              logprob: {\n                type: 'number'\n              },\n              bytes: {\n                type: 'array',\n                items: {\n                  type: 'integer'\n                }\n              }\n            },\n            required: [              'token',\n              'logprob'\n            ]\n          }\n        },\n        bytes: {\n          type: 'array',\n          items: {\n            type: 'integer'\n          }\n        }\n      },\n      required: [        'token',\n        'logprob',\n        'top_logprobs'\n      ]\n    }\n  }\n}\n```",
  inputSchema: {
    type: 'object',
    anyOf: [
      {
        type: 'object',
        properties: {
          model: {
            type: 'string',
            description:
              'The [ID of the model](https://dev.writer.com/home/models) to use for generating text. Supports `palmyra-x5`, `palmyra-x4`, `palmyra-fin`, `palmyra-med`, `palmyra-creative`, and `palmyra-x-003-instruct`.',
          },
          prompt: {
            type: 'string',
            description: 'The input text that the model will process to generate a response.',
          },
          best_of: {
            type: 'integer',
            description:
              'Specifies the number of completions to generate and return the best one. Useful for generating multiple outputs and choosing the best based on some criteria.',
          },
          max_tokens: {
            type: 'integer',
            description: 'The maximum number of tokens that the model can generate in the response.',
          },
          random_seed: {
            type: 'integer',
            description:
              'A seed used to initialize the random number generator for the model, ensuring reproducibility of the output when the same inputs are provided.',
          },
          stop: {
            anyOf: [
              {
                type: 'array',
                items: {
                  type: 'string',
                },
              },
              {
                type: 'string',
              },
            ],
            description:
              "Specifies stopping conditions for the model's output generation. This can be an array of strings or a single string that the model will look for as a signal to stop generating further tokens.",
          },
          stream: {
            type: 'string',
            description:
              "Determines whether the model's output should be streamed. If true, the output is generated and sent incrementally, which can be useful for real-time applications.",
            enum: [false],
          },
          temperature: {
            type: 'number',
            description:
              "Controls the randomness of the model's outputs. Higher values lead to more random outputs, while lower values make the model more deterministic.",
          },
          top_p: {
            type: 'number',
            description:
              'Used to control the nucleus sampling, where only the most probable tokens with a cumulative probability of top_p are considered for sampling, providing a way to fine-tune the randomness of predictions.',
          },
        },
        required: ['model', 'prompt'],
      },
      {
        type: 'object',
        properties: {
          model: {
            type: 'string',
            description:
              'The [ID of the model](https://dev.writer.com/home/models) to use for generating text. Supports `palmyra-x5`, `palmyra-x4`, `palmyra-fin`, `palmyra-med`, `palmyra-creative`, and `palmyra-x-003-instruct`.',
          },
          prompt: {
            type: 'string',
            description: 'The input text that the model will process to generate a response.',
          },
          stream: {
            type: 'string',
            description:
              "Determines whether the model's output should be streamed. If true, the output is generated and sent incrementally, which can be useful for real-time applications.",
            enum: [true],
          },
          best_of: {
            type: 'integer',
            description:
              'Specifies the number of completions to generate and return the best one. Useful for generating multiple outputs and choosing the best based on some criteria.',
          },
          max_tokens: {
            type: 'integer',
            description: 'The maximum number of tokens that the model can generate in the response.',
          },
          random_seed: {
            type: 'integer',
            description:
              'A seed used to initialize the random number generator for the model, ensuring reproducibility of the output when the same inputs are provided.',
          },
          stop: {
            anyOf: [
              {
                type: 'array',
                items: {
                  type: 'string',
                },
              },
              {
                type: 'string',
              },
            ],
            description:
              "Specifies stopping conditions for the model's output generation. This can be an array of strings or a single string that the model will look for as a signal to stop generating further tokens.",
          },
          temperature: {
            type: 'number',
            description:
              "Controls the randomness of the model's outputs. Higher values lead to more random outputs, while lower values make the model more deterministic.",
          },
          top_p: {
            type: 'number',
            description:
              'Used to control the nucleus sampling, where only the most probable tokens with a cumulative probability of top_p are considered for sampling, providing a way to fine-tune the randomness of predictions.',
          },
        },
        required: ['model', 'prompt', 'stream'],
      },
    ],
    properties: {
      jq_filter: {
        type: 'string',
        title: 'jq Filter',
        description:
          'A jq filter to apply to the response to include certain fields. Consult the output schema in the tool description to see the fields that are available.\n\nFor example: to include only the `name` field in every object of a results array, you can provide ".results[].name".\n\nFor more information, see the [jq documentation](https://jqlang.org/manual/).',
      },
    },
  },
  annotations: {},
};

export const handler = async (client: Writer, args: Record<string, unknown> | undefined) => {
  const { jq_filter, ...body } = args as any;
  return asTextContentResult(await maybeFilter(jq_filter, await client.completions.create(body)));
};

export default { metadata, tool, handler };
